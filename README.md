---

title: Emotion Detection Webcam
emoji: ğŸ¥
colorFrom: purple
colorTo: pink
sdk: gradio
app_file: app.py
pinned: false
-------------

# ğŸš€ DÃ©tection dâ€™Ã‰motions en Temps RÃ©el via Webcam

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

Ce projet permet de dÃ©tecter les Ã©motions humaines en temps rÃ©el Ã  partir du flux vidÃ©o dâ€™une webcam. Il combine des techniques de vision par ordinateur (dÃ©tection de visage et points de repÃ¨re faciaux) et des modÃ¨les de Machine Learning pour classifier lâ€™Ã©motion observÃ©e.

---

## ğŸ“‹ Table des matiÃ¨res

1. FonctionnalitÃ©s
2. Technologies utilisÃ©es
3. PrÃ©requis
4. Installation
5. EntraÃ®nement des modÃ¨les
6. Utilisation
7. Fonctionnement interne
8. Structure du projet
9. Contribution
10. Licence

---

## âœ¨ FonctionnalitÃ©s

* DÃ©tection de visage en temps rÃ©el via la webcam
* Extraction de 68 points de repÃ¨re faciaux (facial landmarks)
* Classification automatique des Ã©motions
* Choix du modÃ¨le de prÃ©diction :

  * Support Vector Machine (SVM)
  * Random Forest
* Affichage en direct de lâ€™Ã©motion dÃ©tectÃ©e sur le flux vidÃ©o

---

## ğŸ› ï¸ Technologies utilisÃ©es

* Python 3.8+
* OpenCV (capture et traitement vidÃ©o)
* Dlib (dÃ©tection de visage et landmarks)
* Scikit-learn (SVM et Random Forest)
* Joblib (sauvegarde et chargement des modÃ¨les)
* NumPy (calculs numÃ©riques)
* Jupyter Notebook (entraÃ®nement et expÃ©rimentation)

---

## ğŸ›‘ PrÃ©requis

Avant de lancer le projet, tÃ©lÃ©chargez le modÃ¨le Dlib des points de repÃ¨re faciaux :

* shape_predictor_68_face_landmarks.dat
  [http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2](http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2)

DÃ©compressez le fichier et placez `shape_predictor_68_face_landmarks.dat` Ã  la racine du projet.

---

## ğŸ”§ Installation

1. Cloner le dÃ©pÃ´t

```bash
git clone https://github.com/<VOTRE-USERNAME>/emotion-detection-webcam.git
cd emotion-detection-webcam
```

2. CrÃ©er un environnement virtuel (recommandÃ©)

```bash
python -m venv venv
source venv/bin/activate  # Windows : venv\Scripts\activate
```

3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

Contenu du fichier `requirements.txt` :

```txt
opencv-python
dlib
scikit-learn==1.3.0
numpy
```

---

## ğŸ§ª EntraÃ®nement des modÃ¨les

Les fichiers `.joblib` ne sont pas versionnÃ©s dans le dÃ©pÃ´t. Les modÃ¨les doivent Ãªtre entraÃ®nÃ©s localement.

1. Lancer Jupyter Notebook

```bash
jupyter notebook
```

2. ExÃ©cuter les notebooks suivants :

* `SVM_WEBCAM_FINAL.ipynb` (modÃ¨le SVM)
* `AMELIORATION_MODELE_WEBCAM.ipynb` (modÃ¨le Random Forest)

Les modÃ¨les entraÃ®nÃ©s seront sauvegardÃ©s dans les dossiers correspondants.

---

## â–¶ï¸ Utilisation

Assurez-vous que votre webcam est connectÃ©e et fonctionnelle.

* Lancer la dÃ©tection avec le modÃ¨le SVM

```bash
python webcamwithmodelsvm.py
```

* Lancer la dÃ©tection avec le modÃ¨le Random Forest

```bash
python webcamwithmodelrandomforest.py
```

Appuyez sur la touche **q** pour quitter lâ€™application.

---

## ğŸ§  Fonctionnement interne

1. Capture du flux vidÃ©o depuis la webcam
2. DÃ©tection du visage avec Dlib
3. Extraction des 68 points de repÃ¨re faciaux
4. Normalisation des coordonnÃ©es et rÃ©duction de dimension (ACP)
5. PrÃ©diction de lâ€™Ã©motion via le modÃ¨le sÃ©lectionnÃ©
6. Affichage de lâ€™Ã©motion et du cadre du visage en temps rÃ©el

---

## ğŸ“‚ Structure du projet

```
.
â”œâ”€â”€ AMELIORATION_MODELE_WEBCAM.ipynb
â”œâ”€â”€ SVM_WEBCAM_FINAL.ipynb
â”œâ”€â”€ webcamwithmodelsvm.py
â”œâ”€â”€ webcamwithmodelrandomforest.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ shape_predictor_68_face_landmarks.dat
```

---

## ğŸ¤ Contribution

Les contributions sont bienvenues.

1. Forker le projet
2. CrÃ©er une branche (`feature/nom-fonctionnalite`)
3. Commiter les changements
4. Pusher la branche
5. Ouvrir une Pull Request

---

## ğŸ“„ Licence

Ce projet est distribuÃ© sous licence MIT.
